In linear algebra, the Cayley–Hamilton theorem (named after the mathematicians Arthur Cayley and William Rowan Hamilton) states that every square matrix over a commutative ring (such as the real or complex field) satisfies its own characteristic equation.

If A is a given n×n matrix and In  is the n×n identity matrix, then the characteristic polynomial of A is defined as[7]

{\displaystyle p(\lambda )=\det(\lambda I_{n}-A)~,} p(\lambda )=\det(\lambda I_{n}-A)~,
where det is the determinant operation and ? is a scalar element of the base ring. Since the entries of the matrix are (linear or constant) polynomials in ?, the determinant is also an n-th order monic polynomial in ?. The Cayley–Hamilton theorem states that substituting the matrix A for ? in this polynomial results in the zero matrix,

{\displaystyle p(A)=0.} p(A)=0.
The powers of A, obtained by substitution from powers of ?, are defined by repeated matrix multiplication; the constant term of p(?) gives a multiple of the power A0, which is defined as the identity matrix. The theorem allows An to be expressed as a linear combination of the lower matrix powers of A. When the ring is a field, the Cayley–Hamilton theorem is equivalent to the statement that the minimal polynomial of a square matrix divides its characteristic polynomial.

The theorem was first proved in 1853[8] in terms of inverses of linear functions of quaternions, a non-commutative ring, by Hamilton.[4][5][6] This corresponds to the special case of certain 4 × 4 real or 2 × 2 complex matrices. The theorem holds for general quaternionic matrices.[9][nb 1] Cayley in 1858 stated it for 3 × 3 and smaller matrices, but only published a proof for the 2 × 2 case.[2] The general case was first proved by Frobenius in 1878.[10]

The left hand side can be worked out to an n×n matrix whose entries are (enormous) polynomial expressions in the set of entries ai,j of A, so the Cayley–Hamilton theorem states that each of these n2 expressions are equal to 0. For any fixed value of n these identities can be obtained by tedious but completely straightforward algebraic manipulations. None of these computations can show however why the Cayley–Hamilton theorem should be valid for matrices of all possible sizes n, so a uniform proof for all n is needed.

which is the null vector since p(?) = 0 (the eigenvalues of A are precisely the roots of p(t). This holds for all possible eigenvalues ?, so the two matrices equated by the theorem certainly give the same (null) result when applied to any eigenvector. Now if A admits a basis of eigenvectors, in other words if A is diagonalizable, then the Cayley–Hamilton theorem must hold for A, since two matrices that give the same values when applied to each element of a basis must be equal.

This proof is similar to the first one, but tries to give meaning to the notion of polynomial with matrix coefficients that was suggested by the expressions occurring in that proof. This requires considerable care, since it is somewhat unusual to consider polynomials with coefficients in a non-commutative ring, and not all reasoning that is valid for commutative polynomials can be applied in this setting.

Notably, while arithmetic of polynomials over a commutative ring models the arithmetic of polynomial functions, this is not the case over a non-commutative ring (in fact there is no obvious notion of polynomial function in this case that is closed under multiplication). So when considering polynomials in t with matrix coefficients, the variable t must not be thought of as an "unknown", but as a formal symbol that is to be manipulated according to given rules; in particular one cannot just set t to a specific value.
Let M(n, R) be the ring of n×n matrices with entries in some ring R (such as the real or complex numbers) that has A as an element. Matrices with as coefficients polynomials in t, such as {\displaystyle tI_{n}-A} tI_{n}-A or its adjugate B in the first proof, are elements of M(n, R[t]).

By collecting like powers of t, such matrices can be written as "polynomials" in t with constant matrices as coefficients; write M(n, R)[t] for the set of such polynomials. Since this set is in bijection with M(n, R[t]), one defines arithmetic operations on it correspondingly, in particular multiplication is given by
At this point, it is tempting to simply set t equal to the matrix A , which makes the first factor on the left equal to the null matrix, and the right hand side equal to p(A); however, this is not an allowed operation when coefficients do not commute. It is possible to define a "right-evaluation map" evA : M[t] ? M, which replaces each ti by the matrix power Ai of A , where one stipulates that the power is always to be multiplied on the right to the corresponding coefficient.

But this map is not a ring homomorphism: the right-evaluation of a product differs in general from the product of the right-evaluations. This is so because multiplication of polynomials with matrix coefficients does not model multiplication of expressions containing unknowns: a product {\displaystyle Mt^{i}Nt^{j}=(M\cdot N)t^{i+j}} Mt^{i}Nt^{j}=(M\cdot N)t^{i+j} is defined assuming that t commutes with N, but this may fail if t is replaced by the matrix A.

One can work around this difficulty in the particular situation at hand, since the above right-evaluation map does become a ring homomorphism if the matrix A is in the center of the ring of coefficients, so that it commutes with all the coefficients of the polynomials (the argument proving this is straightforward, exactly because commuting t with coefficients is now justified after evaluation).

Now, A is not always in the center of M, but we may replace M with a smaller ring provided it contains all the coefficients of the polynomials in question: {\displaystyle I_{n}} I_{n}, A, and the coefficients {\displaystyle B_{i}} B_{i} of the polynomial B. The obvious choice for such a subring is the centralizer Z of A, the subring of all matrices that commute with A; by definition A is in the center of Z.

This centralizer obviously contains {\displaystyle I_{n}} I_{n}, and A, but one has to show that it contains the matrices {\displaystyle B_{i}} B_{i}. To do this, one combines the two fundamental relations for adjugates, writing out the adjugate B as a polynomial:
In the first proof, one was able to determine the coefficients Bi of B based on the right-hand fundamental relation for the adjugate only. In fact the first n equations derived can be interpreted as determining the quotient B of the Euclidean division of the polynomial p(t)In on the left by the monic polynomial Int-A, while the final equation expresses the fact that the remainder is zero. This division is performed in the ring of polynomials with matrix coefficients. Indeed, even over a non-commutative ring, Euclidean division by a monic polynomial P is defined, and always produces a unique quotient and remainder with the same degree condition as in the commutative case, provided it is specified at which side one wishes P to be a factor (here that is to the left).

To see that quotient and remainder are unique (which is the important part of the statement here), it suffices to write {\displaystyle PQ+r=PQ'+r'} PQ+r=PQ'+r' as {\displaystyle P(Q-Q')=r'-r} P(Q-Q')=r'-r and observe that since P is monic, P(Q-Q') cannot have a degree less than that of P, unless Q=Q' .

But the dividend p(t)In and divisor Int-A used here both lie in the subring (R[A])[t], where R[A] is the subring of the matrix ring M(n, R) generated by A: the R-linear span of all powers of A. Therefore, the Euclidean division can in fact be performed within that commutative polynomial ring, and of course it then gives the same quotient B and remainder 0 as in the larger ring; in particular this shows that B in fact lies in (R[A])[t].

But, in this commutative setting, it is valid to set t to A in the equation
In addition to proving the theorem, the above argument tells us that the coefficients Bi of B are polynomials in A, while from the second proof we only knew that they lie in the centralizer Z of A; in general Z is a larger subring than R[A], and not necessarily commutative. In particular the constant term B0= adj(-A) lies in R[A]. Since A is an arbitrary square matrix, this proves that adj(A) can always be expressed as a polynomial in A (with coefficients that depend on A).

In fact, the equations found in the first proof allow successively expressing {\displaystyle B_{n-1},\ldots ,B_{1},B_{0}} B_{n-1},\ldots ,B_{1},B_{0} as polynomials in A, which leads to the identity
As was mentioned above, the matrix p(A) in statement of the theorem is obtained by first evaluating the determinant and then substituting the matrix A for t; doing that substitution into the matrix {\displaystyle tI_{n}-A} tI_{n}-A before evaluating the determinant is not meaningful. Nevertheless, it is possible to give an interpretation where p(A) is obtained directly as the value of a certain determinant, but this requires a more complicated setting, one of matrices over a ring in which one can interpret both the entries {\displaystyle A_{i,j}} A_{i,j} of A, and all of A itself. One could take for this the ring M(n, R) of n×n matrices over R, where the entry {\displaystyle A_{i,j}} A_{i,j} is realised as {\displaystyle A_{i,j}I_{n}} A_{i,j}I_{n}, and A as itself. But considering matrices with matrices as entries might cause confusion with block matrices, which is not intended, as that gives the wrong notion of determinant (recall that the determinant of a matrix is defined as a sum of products of its entries, and in the case of a block matrix this is generally not the same as the corresponding sum of products of its blocks!). It is clearer to distinguish A from the endomorphism f of an n-dimensional vector space V (or free R-module if R is not a field) defined by it in a basis e1, ..., en, and to take matrices over the ring End(V) of all such endomorphisms. Then f ? End(V) is a possible matrix entry, while A designates the element of M(n, End(V)) whose i,j entry is endomorphism of scalar multiplication by {\displaystyle A_{i,j}} A_{i,j}; similarly In will be interpreted as element of M(n, End(V)). However, since End(V) is not a commutative ring, no determinant is defined on M(n, End(V)); this can only be done for matrices over a commutative subring of End(V). Now the entries of the matrix {\displaystyle \varphi I_{n}-A} \varphi I_{n}-A all lie in the subring R[f] generated by the identity and f, which is commutative. Then a determinant map M(n, R[f]) ? R[f] is defined, and {\displaystyle \det(\varphi I_{n}-A)} \det(\varphi I_{n}-A) evaluates to the value p(f) of the characteristic polynomial of A at f (this holds independently of the relation between A and f); the Cayley–Hamilton theorem states that p(f) is the null endomorphism.

In this form, the following proof can be obtained from that of (Atiyah & MacDonald 1969, Prop. 2.4) (which in fact is the more general statement related to the Nakayama lemma; one takes for the ideal in that proposition the whole ring R). The fact that A is the matrix of f in the basis e1, ..., en means that

Not all matrices are diagonalizable, but for matrices with complex coefficients many of them are: the set of diagonalizable complex square matrices of a given size is dense in the set of all such square matrices[16] (for a matrix to be diagonalizable it suffices for instance that its characteristic polynomial not have any multiple roots). Now if any of the n2 expressions that the theorem equates to 0 would not reduce to a null expression, in other words if it would be a nonzero polynomial in the coefficients of the matrix, then the set of complex matrices for which this expression happens to give 0 would not be dense in the set of all matrices, which would contradict the fact that the theorem holds for all diagonalizable matrices. Thus one can see that the Cayley–Hamilton theorem must be true.

While this provides a valid proof (for matrices over the complex numbers), the argument is not very satisfactory, since the identities represented by the theorem do not in any way depend on the nature of the matrix (diagonalizable or not), nor on the kind of entries allowed (for matrices with real entries the diagonalizable ones do not form a dense set, and it seems strange one would have to consider complex matrices to see that the Cayley–Hamilton theorem holds for them). We shall therefore now consider only arguments that prove the theorem directly for any matrix using algebraic manipulations only; these also have the benefit of working for matrices with entries in any commutative ring.

There is a great variety of such proofs of the Cayley–Hamilton theorem, of which several will be given here. They vary in the amount of abstract algebraic notions required to understand the proof. The simplest proofs use just those notions needed to formulate the theorem (matrices, polynomials with numeric entries, determinants), but involve technical computations that render somewhat mysterious the fact that they lead precisely to the correct conclusion. It is possible to avoid such details, but at the price of involving more subtle algebraic notions: polynomials with coefficients in a non-commutative ring, or matrices with unusual kinds of entries.